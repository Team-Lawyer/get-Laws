import logging
import os
import re
import sys
from hashlib import md5
from pathlib import Path
from time import time
from typing import Any, List

from common import LINE_RE
from manager import CacheManager, RequestManager
from parsers import ContentParser, HTMLParser, Parser, WordParser

# ======MongoDB é…ç½®
from pymongo import MongoClient
from urllib.parse import quote_plus

Password = quote_plus("lt3370")
Mongo_URI = f"mongodb+srv://lawtry:{Password}@cluster0.qom7j5h.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"
client = MongoClient(Mongo_URI)

# =====Logger é…ç½®
logger = logging.getLogger("Law")
logger.setLevel(logging.DEBUG)

# è¾“å‡ºæ ¼å¼
formatter = logging.Formatter("%(asctime)s:%(levelname)s:%(message)s")
# æ§åˆ¶å°è¾“å‡º
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)

logger.addHandler(console_handler)

# æ–‡ä»¶è§£æå™¨ï¼ˆword æˆ–è€…htmlï¼‰
def find(f, arr: List[Any]) -> Any:
    for item in arr:
        if f(item):
            return item
    raise Exception("not found")

# åˆ¤æ–­æŸä¸€è¡Œæ˜¯å¦æ˜¯æ­£æ–‡å¼€å§‹
def isStartLine(line: str):
    for reg in LINE_RE:
        if re.match(reg, line):
            return True
    return False


class LawParser(object):
    def __init__(self) -> None:
        self.request = RequestManager()
        self.spec_title = None
        self.parser = [
            HTMLParser(),
            WordParser(),
        ]
        self.content_parser = ContentParser()
        self.cache = CacheManager()
        self.categories = []

        self.mongo_client = client
        self.db = self.mongo_client["lawdb"]


    def __reorder_files(self, files):
        # å¿…é¡»æœ‰ parser
        files = list(
            filter(
                lambda x: x["type"] in self.parser,
                files,
            )
        )
        # åªä¿ç•™HTML/WORDæ–‡ä»¶ æ’åºååˆ—è¡¨
        if len(files) == 0:
            return []

        if len(files) > 1:
            # æŒ‰ç…§ parser çš„ä½ç½®æ’åºï¼Œ ä¼˜å…ˆä½¿ç”¨çº§åˆ«
            files = sorted(files, key=lambda x: self.parser.index(x["type"]))

        return files

    def is_bypassed_law(self, item) -> bool:
        # ç”¨æ­£åˆ™æ’å‡º å†³å®šã€å¤å‡½ã€æ‰¹å¤ç­‰æ³•å¾‹
        title = item["title"].replace("ä¸­åäººæ°‘å…±å’Œå›½", "")
        if self.spec_title and title in self.spec_title:
            return False
        if re.search(r"çš„(å†³å®š|å¤å‡½|æ‰¹å¤|ç­”å¤|æ‰¹å¤)$", title):
            return True
        return False

    def parse_law(self, item):
        ### æ­£æ–‡æ–‡ä»¶ï¼Œæ‰¾åˆ°åˆé€‚çš„æ ¼å¼HTMLè¿˜æ˜¯DOC è¯»å–å†…å®¹
        detail = self.request.get_law_detail(item["id"])
        # print(item["id"])
        law_id = item["id"]
        result = detail["result"]
        title = result["title"]
        # ç±»åˆ«
        level = result["level"]

        # æ ¹æ® level ç¡®å®š collection åç§° ğŸ™‹ğŸ™‹ğŸ™‹ğŸ™‹ğŸ™‹
        if level == "å®ªæ³•":
            collection_name = "constitutions"
        else:
            collection_name = "laws"

        # è·å– collectionï¼ŒMongoDB ä¼šè‡ªåŠ¨åˆ›å»ºï¼ˆä¸å­˜åœ¨æ—¶ï¼‰
        existing_collections = self.db.list_collection_names()
        collection_exists = collection_name in existing_collections
        collection = self.db[collection_name]

        # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ç›¸åŒ idï¼Œå­˜åœ¨åˆ™è·³è¿‡
        if collection_exists:
            if collection.find_one({"_id": law_id}):
                logger.info(f"Document with id {law_id} name {title} already exists in collection [{collection_name}], skipping.")
                return

        files = self.__reorder_files(result["body"])
        logger.debug(f"parsing {title}")
        if len(files) == 0:
            return
        ### è¯»å– åŒ–æˆä¸‰å…ƒç»„ title, desc, content
        for target_file in files:
            parser: Parser = find(lambda x: x == target_file["type"], self.parser)

            ret = parser.parse(result, target_file)
            if not ret:
                logger.error(f"parsing {title} error")
                continue
            # _, desc, content, content_fonts = ret
            _, desc, content = ret
            # filedata = self.content_parser.parse(result, title, desc, content, law_id, content_fonts)
            filedata = self.content_parser.parse(result, title, desc, content, law_id)
            if not filedata:
                continue
            # å†™å…¥ MongoDB Atlas
            collection.insert_one(filedata)

            # å¦‚éœ€ä¿ç•™æ–‡ä»¶å†™å…¥ï¼Œå¯ç»§ç»­å†™ MD æ–‡ä»¶
            output_path = level / self.__get_law_output_path(title, item["publish"])
            logger.debug(f"parsing {title} success")
            self.cache.write_law(output_path, filedata)
            # print("inserting")
            # # è°ƒç”¨cacheä¸­çš„å†™å…¥å‡½æ•° å†™å…¥ã€‚mdæ–‡ä»¶
            # output_path = level / self.__get_law_output_path(title, item["publish"])
            # logger.debug(f"parsing {title} success")
            # self.cache.write_law(output_path, filedata)

    def parse_file(self, file_path, publish_at=None):
        # ç¦»çº¿è½¬ ä»txtè½¬md
        result = {}
        with open(file_path, "r") as f:
            data = list(filter(lambda x: x, map(lambda x: x.strip(), f.readlines())))
        title = data[0]
        filedata = self.content_parser.parse(result, title, data[1], data[2:])
        if not filedata:
            return
        output_path = self.__get_law_output_path(title, publish_at)
        logger.debug(f"parsing {title} success")
        self.cache.write_law(output_path, filedata)

    def get_file_hash(self, title, publish_at=None) -> str:
        # åŸºäºæ ‡é¢˜å’Œå‘å¸ƒæ—¥æœŸç”Ÿæˆ MD5å“ˆå¸Œå€¼ é™¤é‡
        _hash = md5()
        _hash.update(title.encode("utf8"))
        if publish_at:
            _hash.update(publish_at.encode("utf8"))
        return _hash.digest().hex()[0:8]

    def __get_law_output_path(self, title, publish_at: str) -> Path:
        # æ ¹æ®æ ‡é¢˜ å’Œæ—¥æœŸç”Ÿæˆã€‚mdæ–‡ä»¶
        title = title.replace("ä¸­åäººæ°‘å…±å’Œå›½", "")
        ret = Path(".")
        for category in self.categories:
            if title in category["title"]:
                ret = ret / category["category"]
                break
        # hash_hex = self.get_file_hash(title, publish_at)
        if publish_at:
            output_name = f"{title}({publish_at}).md"
        else:
            output_name = f"{title}.md"
        return ret / output_name

    # def lawList(self):
    #     # éå†60é¡µæ³•å¾‹æ•°æ®ï¼Œé€æ¡è¿”å›
    #     for i in range(1, 60):
    #         ret = self.request.getLawList(i)
    #         arr = ret["result"]["data"]
    #         if len(arr) == 0:
    #             break
    #         yield from arr
    #
    # def run(self):
    #     # æŠ“å–æµç¨‹ è·³è¿‡æ— æ•ˆæ¡ç›® ä¸‹è½½æ­£æ–‡ è§£æ å­˜å‚¨
    #     for i in range(1, 5):
    #         ret = self.request.getLawList(i)
    #         arr = ret["result"]["data"]
    #         if len(arr) == 0:
    #             break
    #         for item in arr:
    #             if "publish" in item and item["publish"]:
    #                 item["publish"] = item["publish"].split(" ")[0]
    #             if self.is_bypassed_law(item):
    #                 continue
    #             # if item["status"] == "9":
    #             # continue
    #             self.parse_law(item)
    #             if self.spec_title is not None:
    #                 exit(1)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æŠ“å–æ§åˆ¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def lawList(self):
        """ç”Ÿæˆå™¨ï¼šè‡ªåŠ¨ç¿»é¡µåˆ°æ¥å£è¿”å›ç©ºæ•°ç»„ä¸ºæ­¢"""
        page = 1
        while True:
            ret = self.request.getLawList(page)
            arr = ret["result"]["data"]
            if not arr:
                break
            yield from arr
            page += 1

    def run(self):
        for item in self.lawList():
            if "publish" in item and item["publish"]:
                item["publish"] = item["publish"].split(" ")[0]
            if self.is_bypassed_law(item):
                continue
            self.parse_law(item)
            if self.spec_title is not None:
                break

    def remove_duplicates(self):
        # é™¤é‡å¤ å› ä¸ºä¸Šé¢ç”¨è¿‡å“ˆå¸Œå€¼
        p = self.cache.OUTPUT_PATH
        lookup = Path("../")
        for file_path in p.glob("*.md"):
            lookup_files = lookup.glob(f"**/**/{file_path.name}")
            lookup_files = filter(lambda x: "scripts" not in x.parts, lookup_files)
            lookup_files = list(lookup_files)
            if len(lookup_files) > 0:
                os.remove(file_path)
                print(f"remove {file_path}")

def build_xlwj_params(codes: list[str]) -> list[tuple[str, str]]:
    """æŠŠ ['01','02',...] æ„é€ æˆå¤šé‡ xlwj å‚æ•°"""
    return [("xlwj", code) for code in codes]


def main():
    req = LawParser()

    # éœ€è¦æŠ“çš„åˆ†ç±»ä»£ç ï¼ˆ01 å®ªæ³•ï¼Œ02â€“08 æ™®é€šæ³•å¾‹/è¡Œæ”¿æ³•è§„/å¸æ³•è§£é‡Š...ï¼‰
    codes = ["01"] # , "02", "03", "04", "05", "06", "07", "08"]
    req.request.params = build_xlwj_params(codes)

    # 1â€“9 ç±»ï¼šæ³•å¾‹ã€è¡Œæ”¿æ³•è§„ã€åœ°æ–¹æ€§æ³•è§„ã€å¸æ³•è§£é‡Š ...
    req.request.searchType = "1,2,3,4,5,6,7,8,9"
    req.request.req_time = int(time() * 1000)

    args = sys.argv[1:]
    if args:
        # ç¦»çº¿æ¨¡å¼ï¼špython scripts/request.py file.txt 2024-01-01
        req.parse_file(Path(args[0]), args[1] if len(args) > 1 else None)
        return

    try:
        req.run()
    except KeyboardInterrupt:
        logger.info("keyboard interrupt")
    finally:
        req.remove_duplicates()

# def main():
#     req = LawParser()
#     args = sys.argv[1:]
#     if args:
#         req.parse_file(args[0], args[1])
#         return
#     req.request.searchType = "1,2,3,4,5,6,7,8,9"
#     # req.request.searchType = 'title;vague'
#     req.request.params = [
#         ("xlwj", "01"),
#         # ("type", "å…¬å®‰éƒ¨è§„ç« ")
#         ("xlwj", ["02", "03", "04", "05", "06", "07", "08"])  # æ³•å¾‹æ³•è§„
#         # ("xlwj", ["07"]),
#         #  ("fgbt", "æ¶ˆé˜²æ³•"),
#         # ("fgxlwj", "xzfg"),  # è¡Œæ”¿æ³•è§„
#         # ('type', 'sfjs'),
#         # ("zdjg", "4028814858a4d78b0158a50f344e0048&4028814858a4d78b0158a50fa2ba004c"), #åŒ—äº¬
#         # ("zdjg", "4028814858b9b8e50158bed591680061&4028814858b9b8e50158bed64efb0065"), #æ²³å—
#         # ("zdjg", "4028814858b9b8e50158bec45e9a002d&4028814858b9b8e50158bec500350031"), # ä¸Šæµ·
#         # ("zdjg", "4028814858b9b8e50158bec5c28a0035&4028814858b9b8e50158bec6abbf0039"), # æ±Ÿè‹
#         # ("zdjg", "4028814858b9b8e50158bec7c42f003d&4028814858b9b8e50158beca3c590041"), # æµ™æ±Ÿ
#         # ("zdjg", "4028814858b9b8e50158bed40f6d0059&4028814858b9b8e50158bed4987a005d"),  # å±±ä¸œ
#         # ("zdjg", "4028814858b9b8e50158bef1d72600b9&4028814858b9b8e50158bef2706800bd"), # é™•è¥¿çœ
#         # (
#         #     "zdjg",
#         #     "4028814858b9b8e50158beda43a50079&4028814858b9b8e50158bedab7ea007d",
#         # ),  # å¹¿ä¸œ
#         # (
#         #     "zdjg",
#         #     "4028814858b9b8e50158bee5863c0091&4028814858b9b8e50158bee9a3aa0095",
#         # )  # é‡åº†
#     ]
#     # req.request.req_time = 1647659481879
#     req.request.req_time = int(time() * 1000)
#     # req.spec_title = "åæœ‰ç»„ç»‡çŠ¯ç½ªæ³•"
#     try:
#         req.run()
#     except KeyboardInterrupt:
#         logger.info("keyboard interrupt")
#     finally:
#         req.remove_duplicates()


if __name__ == "__main__":
    main()
